{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f63ddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xiao\\.conda\\envs\\ga\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import pickle\n",
    "import stanza\n",
    "import umap\n",
    "\n",
    "from typing import Dict, List\n",
    "from functools import partial\n",
    "from transformers import AdamW, BertConfig, BertForSequenceClassification, \\\n",
    "    BertTokenizer, get_constant_schedule_with_warmup, PreTrainedTokenizer\n",
    "from torch.utils.data.sampler import BatchSampler, RandomSampler, Sampler, \\\n",
    "    SequentialSampler, SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, \\\n",
    "    precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from json import load, dump\n",
    "\n",
    "from captum.attr import DeepLift, GuidedBackprop, InputXGradient, Occlusion, \\\n",
    "    Saliency, configure_interpretable_embedding_layer, \\\n",
    "    remove_interpretable_embedding_layer\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertLayer, BertEncoder, BertModel, BertSelfAttention, BertAttention\n",
    ")\n",
    "                                        \n",
    "from transformers.modeling_utils import (\n",
    "    apply_chunking_to_forward\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "#########################################################################\n",
    "##################### Constants/Customization ###########################\n",
    "#########################################################################\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "trained_model = True\n",
    "num_labels = 3\n",
    "class_label = {0: 'US_2018', 1: 'US_2019', 2: 'US_2020'}\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "dataset_name = 'twitter'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#########################################################################\n",
    "##################### Helper Classes/Functions ##########################\n",
    "#########################################################################\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "class SortedSampler(Sampler):\n",
    "    \"\"\"\n",
    "    https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/samplers\n",
    "    /sorted_sampler.html#SortedSampler\n",
    "    Samples elements sequentially, always in the same order.\n",
    "    Args:\n",
    "        data (iterable): Iterable data.\n",
    "        sort_key (callable): Specifies a function of one argument that is\n",
    "        used to extract a\n",
    "            numerical comparison key from each list element.\n",
    "    Example:\n",
    "        >>> list(SortedSampler(range(10), sort_key=lambda i: -i))\n",
    "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, sort_key=identity):\n",
    "        super().__init__(data)\n",
    "        self.data = data\n",
    "        self.sort_key = sort_key\n",
    "        zip_ = [(i, self.sort_key(row)) for i, row in enumerate(self.data)]\n",
    "        zip_ = sorted(zip_, key=lambda r: r[1])\n",
    "        self.sorted_indexes = [item[0] for item in zip_]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sorted_indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class BucketBatchSampler(BatchSampler):\n",
    "    \"\"\" \n",
    "    https://github.com/PetrochukM/PyTorch-NLP/blob/master/torchnlp\n",
    "    /samplers/bucket_batch_sampler.py\n",
    "    `BucketBatchSampler` toggles between `sampler` batches and sorted batches.\n",
    "    Typically, the `sampler` will be a `RandomSampler` allowing the user to\n",
    "    toggle between random batches and sorted batches. A larger\n",
    "    `bucket_size_multiplier` is more sorted and vice versa.\n",
    "\n",
    "    Args:\n",
    "        sampler (torch.data.utils.sampler.Sampler):\n",
    "        batch_size (int): Size of mini-batch.\n",
    "        drop_last (bool): If `True` the sampler will drop the last batch if\n",
    "            its size would be less than `batch_size`.\n",
    "        sort_key (callable, optional): Callable to specify a comparison key\n",
    "        for sorting.\n",
    "        bucket_size_multiplier (int, optional): Buckets are of size\n",
    "            `batch_size * bucket_size_multiplier`.\n",
    "\n",
    "    Example:\n",
    "        >>> from torchnlp.random import set_seed\n",
    "        >>> set_seed(123)\n",
    "        >>>\n",
    "        >>> from torch.utils.data.sampler import SequentialSampler\n",
    "        >>> sampler = SequentialSampler(list(range(10)))\n",
    "        >>> list(BucketBatchSampler(sampler, batch_size=3, drop_last=False))\n",
    "        [[6, 7, 8], [0, 1, 2], [3, 4, 5], [9]]\n",
    "        >>> list(BucketBatchSampler(sampler, batch_size=3, drop_last=True))\n",
    "        [[0, 1, 2], [3,-- 4, 5], [6, 7, 8]]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset: Dataset,\n",
    "                 batch_size,\n",
    "                 collate_fn,\n",
    "                 drop_last=False,\n",
    "                 shuffle=True,\n",
    "                 sort_key=identity,\n",
    "                 bucket_size_multiplier=100):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        if shuffle:\n",
    "            sampler = RandomSampler(dataset)\n",
    "        else:\n",
    "            sampler = SequentialSampler(dataset)\n",
    "\n",
    "        super().__init__(sampler, batch_size, drop_last)\n",
    "        self.sort_key = sort_key\n",
    "        self.collate_fn = collate_fn\n",
    "        self.bucket_sampler = BatchSampler(sampler,\n",
    "                                           min(\n",
    "                                               batch_size *\n",
    "                                               bucket_size_multiplier,\n",
    "                                               len(sampler)),\n",
    "                                           False)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for bucket in self.bucket_sampler:\n",
    "            sorted_sampler = SortedSampler([self.dataset[i] for i in bucket],\n",
    "                                           self.sort_key)\n",
    "            for batch in SubsetRandomSampler(\n",
    "                    list(BatchSampler(sorted_sampler, self.batch_size,\n",
    "                                      self.drop_last))):\n",
    "                tt = self.collate_fn([self.dataset[bucket[i]] for i in batch])\n",
    "                yield self.collate_fn([self.dataset[bucket[i]] for i in batch])\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            return math.ceil(len(self.sampler) / self.batch_size)\n",
    "\n",
    "def collate_sst2(instances: List[Dict],\n",
    "                 tokenizer: PreTrainedTokenizer,\n",
    "                 return_attention_masks: bool = True,\n",
    "                 pad_to_max_length: bool = False,\n",
    "                 device='cuda') -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Tokenize the input text.\n",
    "    Return [token, mask, label]\n",
    "    \"\"\"\n",
    "    \n",
    "    token_ids = [tokenizer.encode(_x['sentence'], max_length=64, truncation=True) for _x in instances]\n",
    "    \n",
    "    if pad_to_max_length:\n",
    "        batch_max_len = 64\n",
    "    else:\n",
    "        batch_max_len = max([len(_s) for _s in token_ids])\n",
    "        \n",
    "    padded_ids_tensor = torch.tensor(\n",
    "        [_s + [tokenizer.pad_token_id] * (batch_max_len - len(_s)) for _s in\n",
    "         token_ids])\n",
    "    \n",
    "    labels = torch.tensor([_x['label'] for _x in instances], dtype=torch.long)\n",
    "\n",
    "    output_tensors = [padded_ids_tensor]\n",
    "    if return_attention_masks:\n",
    "        output_tensors.append(padded_ids_tensor > 0)\n",
    "    output_tensors.append(labels)\n",
    "\n",
    "    return list(_t.to(device) for _t in output_tensors)\n",
    "\n",
    "\n",
    "collate_fn = partial(collate_sst2,\n",
    "                     tokenizer=tokenizer,\n",
    "                     device=device,\n",
    "                     return_attention_masks=True,\n",
    "                     pad_to_max_length=True)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if np.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                        best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                        best * min_delta / 100)\n",
    "\n",
    "def train_model(model: torch.nn.Module,\n",
    "                train_dl: BatchSampler,\n",
    "                dev_dl: BatchSampler,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                scheduler: torch.optim.lr_scheduler.LambdaLR,\n",
    "                n_epochs: int,\n",
    "                labels: int = 2,\n",
    "                early_stopping: EarlyStopping = None) -> (Dict, Dict):\n",
    "\n",
    "    # Define the performance metric\n",
    "    best_val, best_model_weights = {'val_f1': 0}, None\n",
    "\n",
    "    for ep in range(n_epochs):\n",
    "        for batch in tqdm(train_dl, desc='Training'):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss, _ = model(batch[0], attention_mask=batch[1],\n",
    "                            labels=batch[2].long())[:2]\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        val_p, val_r, val_f1, val_loss, _, _ = eval_model(model, dev_dl, labels)\n",
    "        current_val = {\n",
    "            'val_f1': val_f1, 'val_p': val_p, 'val_r': val_r,\n",
    "            'val_loss': val_loss, 'ep': ep\n",
    "        }\n",
    "        print(current_val, flush=True)\n",
    "\n",
    "        # Early stopping\n",
    "        if current_val['val_f1'] > best_val['val_f1']:\n",
    "            best_val = current_val\n",
    "            best_model_weights = model.state_dict()\n",
    "\n",
    "        if early_stopping and early_stopping.step(val_f1):\n",
    "            print('Early stopping...')\n",
    "            break\n",
    "\n",
    "    return best_model_weights, best_val\n",
    "\n",
    "\n",
    "def eval_model(model: torch.nn.Module, test_dl: BatchSampler, labels,\n",
    "               measure=None):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        labels_all = []\n",
    "        logits_all = []\n",
    "        losses = []\n",
    "\n",
    "        for batch in tqdm(test_dl, desc=\"Evaluation\"):\n",
    "            loss, logits_val = model(batch[0], attention_mask=batch[1],\n",
    "                                     labels=batch[2].long())[:2]\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            labels_all += batch[2].detach().cpu().numpy().tolist()\n",
    "            logits_all += logits_val.detach().cpu().numpy().tolist()\n",
    "\n",
    "        prediction = np.argmax(np.asarray(logits_all).reshape(-1, labels),\n",
    "                               axis=-1)\n",
    "\n",
    "        if measure == 'acc':\n",
    "            p, r = None, None\n",
    "            f1 = accuracy_score(labels_all, prediction)\n",
    "\n",
    "        else:\n",
    "            p, r, f1, _ = precision_recall_fscore_support(labels_all,\n",
    "                                                          prediction,\n",
    "                                                          average='macro')\n",
    "            print(confusion_matrix(labels_all, prediction), flush=True)\n",
    "\n",
    "        return p, r, f1, np.mean(losses), labels_all, prediction.tolist()\n",
    "\n",
    "def create_model():\n",
    "    transformer_config = BertConfig.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "    if init_only:\n",
    "        model = BertForSequenceClassification(config=transformer_config).to(device)\n",
    "\n",
    "    else:\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased',\n",
    "            config=transformer_config\n",
    "        ).to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            'params': [p for n, p in param_optimizer if\n",
    "                       not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 0.01\n",
    "        },\n",
    "        {\n",
    "            'params': [p for n, p in param_optimizer if\n",
    "                       any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 0.0\n",
    "        }]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    es = EarlyStopping(patience=patience, percentage=False, mode='max', min_delta=0.0)\n",
    "    scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=0.05)\n",
    "\n",
    "    return model, optimizer, scheduler, es\n",
    "\n",
    "# We need this to generate saliency scores.\n",
    "class BertModelWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    We need this wrapper because the model is expected to output only one item\n",
    "    in the forward function in captum.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super(BertModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input, attention_mask, labels):\n",
    "        output = self.model(input, attention_mask=attention_mask)\n",
    "        return output[0]\n",
    "\n",
    "# We need this to extract gradients.\n",
    "class MyBertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        if encoder_hidden_states is not None:\n",
    "            mixed_key_layer = self.key(encoder_hidden_states)\n",
    "            mixed_value_layer = self.value(encoder_hidden_states)\n",
    "            attention_mask = encoder_attention_mask\n",
    "        else:\n",
    "            mixed_key_layer = self.key(hidden_states)\n",
    "            mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            seq_length = hidden_states.size()[1]\n",
    "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        # Track the gradient of attention_probs\n",
    "        attention_probs.retain_grad()\n",
    "        \n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "        return outputs\n",
    "    \n",
    "class MyBertAttention(BertAttention):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Replace BertSelfAttention with MyBertSelfAttention\n",
    "        self.self = MyBertSelfAttention(config)\n",
    "        \n",
    "class MyBertLayer(BertLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Replace BertAttention with MyBertAttention\n",
    "        self.attention = MyBertAttention(config)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "        # print(attention_output.shape, len(outputs), outputs[0].shape)\n",
    "\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            assert hasattr(\n",
    "                self, \"crossattention\"\n",
    "            ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        # print(layer_output.shape)\n",
    "        outputs = (layer_output,) + outputs\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "class MyBertEncoder(BertEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Replace BertLayer with MyBertLayer\n",
    "        self.layer = nn.ModuleList([MyBertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        \n",
    "        \n",
    "class MyBertModel(BertModel):\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config, add_pooling_layer)\n",
    "        # Replace BertEncoder with MyBertEncoder\n",
    "        self.encoder = MyBertEncoder(config)\n",
    "\n",
    "\n",
    "class MyBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Replace BertModel with MyBertModel\n",
    "        self.bert = MyBertModel(config)\n",
    "\n",
    "def find_longest_300_sentences(longest_tokens):\n",
    "\tlongest_token_dict = []\n",
    "\n",
    "\tfor t in longest_tokens:\n",
    "\t    longest_token_dict.append({\n",
    "\t        'attention_id': t[0],\n",
    "\t        'tokens': tokens[t[0]]\n",
    "\t    })\n",
    "\n",
    "\tfor k in tqdm(range(len(longest_token_dict))):\n",
    "\t    cur_tokens = longest_token_dict[k]['tokens']\n",
    "\t    \n",
    "\t    for j in dataset_test:\n",
    "\t        target_idx = -1\n",
    "\t        target_tokens = j['sentence']\n",
    "\t        target_token_ids = tokenizer.encode(target_tokens, max_length=64, truncation=True)\n",
    "\t        target_tokens = [tokenizer.ids_to_tokens[id] for id in target_token_ids]\n",
    "\t        overlap_rate = len(set(cur_tokens).intersection(set(target_tokens))) / len(set(cur_tokens).union(set(target_tokens)))\n",
    "\t        if overlap_rate > 0.95:\n",
    "\t            target_idx = j['idx']\n",
    "\t            break\n",
    "\t    \n",
    "\t    longest_token_dict[k]['idx'] = target_idx\n",
    "\n",
    "\tfor k in tqdm(range(len(longest_token_dict))):\n",
    "\t    if longest_token_dict[k]['idx'] != -1:\n",
    "\t        continue\n",
    "\n",
    "\t    cur_tokens = longest_token_dict[k]['tokens']\n",
    "\t    \n",
    "\t    for j in dataset_test:\n",
    "\t        target_idx = -1\n",
    "\t        target_tokens = j['sentence']\n",
    "\t        target_token_ids = tokenizer.encode(target_tokens, max_length=64, truncation=True)\n",
    "\t        target_tokens = [tokenizer.ids_to_tokens[id] for id in target_token_ids]\n",
    "\t        overlap_rate = len(set(cur_tokens).intersection(set(target_tokens))) / len(set(cur_tokens).union(set(target_tokens)))\n",
    "\t        if overlap_rate > 0.93:\n",
    "\t            target_idx = j['idx']\n",
    "\t            break\n",
    "\n",
    "\t    longest_token_dict[k]['idx'] = target_idx\n",
    "\n",
    "\treturn longest_token_dict\n",
    "\n",
    "#################### Gradient Sorting List Helpers #####################\n",
    "\n",
    "def reshape_attentions(test_attentions):\n",
    "\t# Each batch has a different dimension, so we need to reshape the arrays\n",
    "\t# (zero padding)\n",
    "\treshaped_attentions = []\n",
    "\treshaped_attention_grads = []\n",
    "\n",
    "\tfor t in tqdm(range(len(test_attentions))):    \n",
    "\t    swapped_tensor = np.swapaxes(test_attentions[t], 0, 1)\n",
    "\t    target_shape = (swapped_tensor.shape[0], num_layers, num_heads, 64, 64)\n",
    "\t        \n",
    "\t    target_tensor = np.zeros(target_shape)\n",
    "\t    cur_shape = swapped_tensor.shape\n",
    "\t    target_tensor[:cur_shape[0],\n",
    "\t                  :cur_shape[1],\n",
    "\t                  :cur_shape[2],\n",
    "\t                  :cur_shape[3],\n",
    "\t                  :cur_shape[4]] = swapped_tensor\n",
    "\t    reshaped_attentions.append(target_tensor)\n",
    "\t    \n",
    "\t    swapped_tensor = np.swapaxes(test_attentions_grad[t], 0, 1)\n",
    "\t    target_tensor = np.zeros(target_shape)\n",
    "\t    cur_shape = swapped_tensor.shape\n",
    "\t    target_tensor[:cur_shape[0],\n",
    "\t                  :cur_shape[1],\n",
    "\t                  :cur_shape[2],\n",
    "\t                  :cur_shape[3],\n",
    "\t                  :cur_shape[4]] = swapped_tensor\n",
    "\t    reshaped_attention_grads.append(target_tensor)\n",
    "\n",
    "\tall_attention_grads = np.concatenate(reshaped_attention_grads, axis=0)\n",
    "\tall_attentions = np.concatenate(reshaped_attentions, axis=0)\n",
    "\treturn all_attention_grads, all_attentions\n",
    "\n",
    "def find_max_grad_sum(all_test_attentions_grads, batch_id, top_k):\n",
    "    \"\"\"\n",
    "    Find the layer and head ID for the attention head having the largest sum of\n",
    "    absolute values of gradients for the given batch.\n",
    "    \n",
    "    Args:\n",
    "        all_test_attentions_grads(array): attention gradients, shape=(200, num_layers, num_heads, d)\n",
    "        batch_id(int): ID of the intersted batch\n",
    "        top_k(int): Number of heads to return.\n",
    "    \"\"\"\n",
    "\n",
    "    cur_means, cur_indexes = [], []\n",
    "\n",
    "    # Find the attention gradient sum \n",
    "    for layer in range(all_test_attentions_grads.shape[1]):\n",
    "        for head in range(all_test_attentions_grads.shape[2]):\n",
    "            cur_index = (layer, head)\n",
    "            cur_mean = np.sum(np.abs(all_test_attentions_grads[batch_id, layer, head]))\n",
    "            \n",
    "            cur_means.append(cur_mean)\n",
    "            cur_indexes.append(cur_index)\n",
    "\n",
    "    # Sort the gradient sum with the index\n",
    "    mean_index = [(cur_means[i], cur_indexes[i]) for i in range(len(cur_means))]\n",
    "    mean_index = sorted(mean_index, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    return mean_index[:top_k]\n",
    "\n",
    "###################### Dependency Parsing Helpers #######################\n",
    "\n",
    "def concatenate_split_words(lst):\n",
    "    it = iter(lst)\n",
    "    prev = next(it)\n",
    "    tmp = prev\n",
    "    for ele in it:\n",
    "        if ele[:2] != '##':\n",
    "            yield tmp\n",
    "            tmp = ele\n",
    "        else:\n",
    "            tmp += ele[2:]\n",
    "        prev = ele\n",
    "    yield tmp\n",
    "    \n",
    "def build_token_word_offset_mapping(padding_stripped_list):\n",
    "    consecutive_split_word_count = 0\n",
    "    total_split_word_count = 0\n",
    "    split_word_idxs_list = []\n",
    "    for i, token in enumerate(padding_stripped_list):\n",
    "        if token[:2] == '##':\n",
    "            if padding_stripped_list[i + 1][:2] == '##':\n",
    "                consecutive_split_word_count += 1\n",
    "                total_split_word_count += 1\n",
    "                continue\n",
    "            consecutive_split_word_count += 1\n",
    "            total_split_word_count += 1\n",
    "            split_word_idxs_list.append((i - total_split_word_count, consecutive_split_word_count))\n",
    "        else:\n",
    "            consecutive_split_word_count = 0\n",
    "    return split_word_idxs_list\n",
    "\n",
    "def rematch_attentions():\n",
    "\tlongest_token_dict = load(open('./outputs/' + dataset_name + '-longest-300-id.json', 'r'))\n",
    "\n",
    "\ttoken_to_attention_id = {}\n",
    "\tfor i, t in enumerate(tokens):\n",
    "\t    token_to_attention_id[frozenset(t)] = i\n",
    "\t    \n",
    "\tfounded_tokens = []\n",
    "\n",
    "\tfor i, t in tqdm(enumerate(longest_token_dict)):\n",
    "\t    if (frozenset(t['tokens']) in token_to_attention_id):\n",
    "\t        cur_attention_id = token_to_attention_id[frozenset(t['tokens'])]\n",
    "\t        founded_tokens.append(cur_attention_id)\n",
    "\t        longest_token_dict[i]['attention_id'] = cur_attention_id\n",
    "\n",
    "\treturn longest_token_dict\n",
    "\n",
    "#################### Syntactic Sorting List Helpers #####################\n",
    "\n",
    "# Code for evaluating individual attention maps and baselines\n",
    "\n",
    "def evaluate_predictor(prediction_fn, dependency_tagged_list):\n",
    "    \"\"\"Compute accuracies for each relation for the given predictor.\"\"\"\n",
    "    n_correct, n_incorrect = Counter(), Counter()\n",
    "    for example in dependency_tagged_list:\n",
    "        words = example[\"words\"]\n",
    "        predictions = prediction_fn(example)\n",
    "        for i, (p, y, r) in enumerate(zip(predictions, example[\"heads\"],\n",
    "                                          example[\"tags\"])):\n",
    "            is_correct = (p == y)\n",
    "            if r == \"poss\" and p < len(words):\n",
    "            # Special case for poss (see discussion in Section 4.2)\n",
    "                if i < len(words) and words[i + 1] == \"'s\" or words[i + 1] == \"s'\":\n",
    "                    is_correct = (predictions[i + 1] == y)\n",
    "            if is_correct:\n",
    "                n_correct[r] += 1\n",
    "                n_correct[\"all\"] += 1\n",
    "            else:\n",
    "                n_incorrect[r] += 1\n",
    "                n_incorrect[\"all\"] += 1\n",
    "    return {k: n_correct[k] / float(n_correct[k] + n_incorrect[k])\n",
    "            for k in n_incorrect.keys()}\n",
    "\n",
    "def attn_head_predictor(layer, head, mode=\"normal\"):\n",
    "    \"\"\"Assign each word the most-attended-to other word as its head.\"\"\"\n",
    "    def predict(example):\n",
    "        attn = np.array(example[\"attentions\"][layer][head])\n",
    "        if mode == \"transpose\":\n",
    "            attn = attn.T\n",
    "        elif mode == \"both\":\n",
    "            attn += attn.T\n",
    "        else:\n",
    "            assert mode == \"normal\"\n",
    "        # ignore attention to self and [CLS]/[SEP] tokens\n",
    "        attn[range(attn.shape[0]), range(attn.shape[0])] = 0\n",
    "        attn = attn[1:-1, 1:-1]\n",
    "        return np.argmax(attn, axis=-1) + 1  # +1 because ROOT is at index 0\n",
    "    return predict\n",
    "\n",
    "def offset_predictor(offset):\n",
    "    \"\"\"Simple baseline: assign each word the word a fixed offset from\n",
    "    it (e.g., the word to its right) as its head.\"\"\"\n",
    "    def predict(example):\n",
    "        return [max(0, min(i + offset + 1, len(example[\"words\"])))\n",
    "                for i in range(len(example[\"words\"]))]\n",
    "    return predict\n",
    "\n",
    "def get_scores(dependency_tagged_list, mode=\"normal\"):\n",
    "    \"\"\"Get the accuracies of every attention head.\"\"\"\n",
    "    scores = defaultdict(dict)\n",
    "    for layer in range(num_layers):\n",
    "        for head in range(num_heads):\n",
    "            scores[layer][head] = evaluate_predictor(\n",
    "                attn_head_predictor(layer, head, mode), dependency_tagged_list)\n",
    "    return scores\n",
    "\n",
    "def get_all_scores(reln):\n",
    "    \"\"\"Get all attention head scores for a particular relation.\"\"\"\n",
    "    all_scores = []\n",
    "    for key, layer_head_scores in attn_head_scores.items():\n",
    "        for layer, head_scores in layer_head_scores.items():\n",
    "            for head, scores in head_scores.items():\n",
    "                all_scores.append((scores[reln], layer, head, key))\n",
    "    return sorted(all_scores, reverse=True)\n",
    "\n",
    "######################## Atlas List Helpers #############################\n",
    "\n",
    "def generate_mean_semantic_list():\n",
    "\tsemantic_sort_list = load(open('./outputs/' + dataset_name + '-sorted-saliency-heads.json', 'r'))\n",
    "\n",
    "\t# Take the average rank of heads for different sentences\n",
    "\tattention_semantic_rank = {}\n",
    "\tfor i in range(num_layers):\n",
    "\t    for j in range(num_heads):\n",
    "\t        attention_semantic_rank[(i, j)] = {'similarity_scores': []}\n",
    "\n",
    "\tfor k in semantic_sort_list:\n",
    "\t    for s in semantic_sort_list[k]:\n",
    "\t        attention_semantic_rank[(s[1][0], s[1][1])]['similarity_scores'].append(s[0])\n",
    "\n",
    "\tmean_semantic_scores = np.zeros((num_layers, num_heads))\n",
    "\tfor i in range(num_layers):\n",
    "\t    for j in range(num_heads):\n",
    "\t        mean_semantic_scores[i, j] = np.mean(attention_semantic_rank[(i, j)]['similarity_scores'])\n",
    "\n",
    "\treturn mean_semantic_scores\n",
    "\n",
    "def generate_mean_syntactic_list():\n",
    "\tsyntactic_sort_list = load(open('./outputs/' + dataset_name + '-sorted-syntactic-heads.json', 'r'))\n",
    "\n",
    "\tattention_syntactic_rank = {}\n",
    "\tfor i in range(num_layers):\n",
    "\t    for j in range(num_heads):\n",
    "\t        attention_syntactic_rank[(i, j)] = {'accs': []}\n",
    "\t        \n",
    "\tfor k in syntactic_sort_list:\n",
    "\t    for h in syntactic_sort_list[k]['top_heads']:\n",
    "\t        attention_syntactic_rank[(h['head'][0], h['head'][1])]['accs'].append(h['acc'])\n",
    "\n",
    "\tmean_syntactic_scores = np.zeros((num_layers, num_heads))\n",
    "\tfor i in range(num_layers):\n",
    "\t    for j in range(num_heads):\n",
    "\t        cur_array = attention_syntactic_rank[(i, j)]['accs']\n",
    "\t        if len(cur_array) == 0:\n",
    "\t            mean_syntactic_scores[i, j] = 0\n",
    "\t        else:\n",
    "\t            mean_syntactic_scores[i, j] = np.max(cur_array)\n",
    "\n",
    "\tmean_syntactic_scores = np.nan_to_num(mean_syntactic_scores, 0)\n",
    "\treturn mean_syntactic_scores\n",
    "\n",
    "def generate_mean_gradient_list():\n",
    "\tgrad_sort_list = load(open('./outputs/' + dataset_name + '-sorted-grad-heads.json', 'r'))\n",
    "\n",
    "\tattention_grad_rank = {}\n",
    "\tfor i in range(num_layers):\n",
    "\t    for j in range(num_heads):\n",
    "\t        attention_grad_rank[(i, j)] = {'grads': []}\n",
    "\t        \n",
    "\tfor k in grad_sort_list:\n",
    "\t    for h in grad_sort_list[k]:\n",
    "\t        attention_grad_rank[(h[1][0], h[1][1])]['grads'].append(h[0])\n",
    "\n",
    "\tvariances = []\n",
    "\tfor i in range(num_layers):\n",
    "\t    for j in range(num_heads):\n",
    "\t        variances.append(np.std(attention_grad_rank[(i, j)]['grads']))\n",
    "\n",
    "\tmean_grad_scores = np.zeros((num_layers, num_heads))\n",
    "\tfor i in range(num_layers):\n",
    "\t    for j in range(num_heads):\n",
    "\t        mean_grad_scores[i, j] = np.mean(attention_grad_rank[(i, j)]['grads'])\n",
    "\n",
    "\treturn mean_grad_scores\n",
    "\n",
    "def generate_mean_confidence_list():\n",
    "\tconfidence_scores = load(open('./outputs/' + dataset_name + '-mean-confidence-heads.json', 'r'))\n",
    "\n",
    "\tmean_confidence_scores = np.zeros((12, 12))\n",
    "\n",
    "\tfor t in confidence_scores:\n",
    "\t    mean_confidence_scores[t[1][0], t[1][1]] = t[0]\n",
    "\n",
    "\treturn mean_confidence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fcf3fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration data-5ac3a2b961f6d61b\n",
      "Reusing dataset csv (C:\\Users\\Xiao\\.cache\\huggingface\\datasets\\csv\\data-5ac3a2b961f6d61b\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    }
   ],
   "source": [
    "dataset_test = load_dataset('./data', split='train[:3%]')\n",
    "if not trained_model:\n",
    "\tprint(\"1. Training Model...\")\n",
    "\tseed = 202136\n",
    "\trandom.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed)\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "\t# Hyper-parameters\n",
    "\tinit_only = False\n",
    "\tlr = 2e-5\n",
    "\tbatch_size = 8\n",
    "\tepochs = 10\n",
    "\tpatience = 4\n",
    "\n",
    "\t# Load data\n",
    "\tdataset_vali = load_dataset('./data', split='train[3%:6%]')\n",
    "\tdataset_train = load_dataset('./data', split='train[6%:]')\n",
    "\ttest_dl = BucketBatchSampler(batch_size=batch_size,\n",
    "\t                             sort_key=lambda x: len(x['sentence']),\n",
    "\t                             dataset=dataset_test,\n",
    "\t                             collate_fn=collate_fn)\n",
    "\n",
    "\tmodel, optimizer, scheduler, es = create_model()\n",
    "\n",
    "\ttrain_dl = BucketBatchSampler(batch_size=batch_size,\n",
    "\t                              sort_key=lambda x: len(x['sentence']),\n",
    "\t                              dataset=dataset_train,\n",
    "\t                              collate_fn=collate_fn)\n",
    "\n",
    "\tdev_dl = BucketBatchSampler(batch_size=batch_size,\n",
    "\t                            sort_key=lambda x: len(x['sentence']),\n",
    "\t                            dataset=dataset_vali,\n",
    "\t                            collate_fn=collate_fn)\n",
    "\n",
    "\tnum_train_optimization_steps = int(epochs * len(train_dl) / batch_size)\n",
    "\n",
    "\tif init_only:\n",
    "\t    best_model_w, best_perf = model.state_dict(), {'val_f1': 0}\n",
    "\t    \n",
    "\telse:\n",
    "\t    best_model_w, best_perf = train_model(model, train_dl, dev_dl,\n",
    "\t                                          optimizer, scheduler,\n",
    "\t                                          epochs, num_labels, es)\n",
    "\n",
    "\tcheckpoint = {\n",
    "\t    'performance': best_perf,\n",
    "\t    'model': best_model_w\n",
    "\t}\n",
    "\n",
    "\tprint(best_perf)\n",
    "\n",
    "\ttorch.save(checkpoint, './outputs/saved-bert-' + dataset_name + '.pt')\n",
    "\n",
    "\tprint('F1', best_perf['val_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0079a77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Extracting Attention Weights and Gradients...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m test_softmaxes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(test_dl):\n\u001b[0;32m     25\u001b[0m     tokens, masks, labels \u001b[38;5;241m=\u001b[39m (batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     26\u001b[0m                              batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     27\u001b[0m                              batch[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Compute the attetion and gradients\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\ga\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mBucketBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bucket \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbucket_sampler:\n\u001b[1;32m--> 157\u001b[0m         sorted_sampler \u001b[38;5;241m=\u001b[39m \u001b[43mSortedSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m SubsetRandomSampler(\n\u001b[0;32m    160\u001b[0m                 \u001b[38;5;28mlist\u001b[39m(BatchSampler(sorted_sampler, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m    161\u001b[0m                                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_last))):\n\u001b[0;32m    162\u001b[0m             tt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[bucket[i]] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch])\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mSortedSampler.__init__\u001b[1;34m(self, data, sort_key)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_key \u001b[38;5;241m=\u001b[39m sort_key\n\u001b[0;32m     88\u001b[0m zip_ \u001b[38;5;241m=\u001b[39m [(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_key(row)) \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)]\n\u001b[1;32m---> 89\u001b[0m zip_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mzip_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msorted_indexes \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m zip_]\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "print(\"2. Extracting Attention Weights and Gradients...\")\n",
    "batch_size = 16\n",
    "test_dl = BucketBatchSampler(batch_size=batch_size,\n",
    "                            dataset=dataset_test,\n",
    "                             collate_fn=collate_fn)\n",
    "\n",
    "transformer_config = BertConfig.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "my_model = MyBertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased',\n",
    "            config=transformer_config\n",
    "        ).to(device)\n",
    "\n",
    "checkpoint = torch.load('./outputs/saved-bert-'  + dataset_name + '.pt')\n",
    "my_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "# Retrieve model output\n",
    "token_ids = []\n",
    "test_attentions = []\n",
    "test_attentions_grad = []\n",
    "test_predicts = []\n",
    "test_labels = []\n",
    "test_softmaxes = []\n",
    "for batch in tqdm(test_dl):\n",
    "    tokens, masks, labels = (batch[0].to(device),\n",
    "                             batch[1].to(device),\n",
    "                             batch[2].to(device))\n",
    "    \n",
    "    # Compute the attetion and gradients\n",
    "    my_loss, my_logit, attentions = my_model(tokens,\n",
    "                                             attention_mask=masks,\n",
    "                                             labels=labels.long(),\n",
    "                                             output_attentions=True)\n",
    "    \n",
    "    softmax_scores = functional.softmax(my_logit, dim=1)\n",
    "    my_predicts = torch.argmax(softmax_scores, dim=1).detach().cpu().numpy().tolist()\n",
    "    test_predicts.extend(my_predicts)\n",
    "    test_labels.extend(labels.detach().cpu().numpy().tolist())\n",
    "    test_softmaxes.extend(softmax_scores.detach().cpu().numpy().tolist())\n",
    "    \n",
    "    my_loss.backward()\n",
    "    \n",
    "    # Attention dimension [num_layer, batch_size, num_head, token_size, token_size]\n",
    "    all_attention = np.array([a.detach().cpu().numpy() for a in attentions])\n",
    "    test_attentions.append(all_attention)\n",
    "    \n",
    "    # Attention gradient dimension [num_layer, batch_size, num_head, token_size, token_size]\n",
    "    all_attention_grad = np.array([a.grad.detach().cpu().numpy() for a in attentions])\n",
    "    test_attentions_grad.append(all_attention_grad)\n",
    "    \n",
    "    # Store the current tokens\n",
    "    token_ids += batch[0].detach().cpu().numpy().tolist()\n",
    "\n",
    "tokens = []\n",
    "for i in range(len(token_ids)):\n",
    "    cur_token_ids = token_ids[i]\n",
    "    cur_tokens = [tokenizer.ids_to_tokens[id] for id in cur_token_ids]\n",
    "    tokens.append(cur_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a3dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880f1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
